# ğŸ“˜ PDF-RAG-App  
### PDF Q&A Chatbot using Ollama + LangChain + FAISS

This project is a **Retrieval-Augmented Generation (RAG)** application that allows users to:

- Upload any **PDF**
- Ask questions about its content  
- Generate **summaries** (short, detailed, bullet points)
- View the **source pages** used for each answer  
- Run everything **locally for free** using **Ollama LLaMA 3**  
- Enjoy a clean UI powered by **Streamlit**

---

## ğŸš€ Features

- ğŸ“„ **PDF Text Extraction** with PyPDF2  
- âœ‚ï¸ **Smart Text Chunking** using LangChain  
- ğŸ§  **Embeddings** via OLLAMA nomic-embed-text  
- ğŸ“š **Vector Search** with FAISS  
- ğŸ¤– **Local LLM** (LLaMA 3) through Ollama  
- ğŸ’¬ **Chat Mode** + Chat History  
- ğŸ“ **Summary Generator** (short / detailed / bullet point)  
- ğŸ” Fully local â€” no API keys required  

---

## ğŸ—ï¸ Project Structure

â”‚â”€â”€ app.py # Streamlit UI

â”‚â”€â”€ rag_pipeline.py # All RAG logic (clean architecture)

â”‚â”€â”€ requirements.txt # Python dependencies

â”‚â”€â”€ Dockerfile # Deployment-ready container

â”‚â”€â”€ .gitignore

â”‚â”€â”€ .env.example

â”‚â”€â”€ Images/ # Screenshots (optional)


---

## â–¶ï¸ Run Locally (Recommended)

1ï¸âƒ£ Install dependencies  

pip install -r requirements.txt

2ï¸âƒ£ Start Ollama

Download Ollama: https://ollama.ai

ollama pull llama3.1:8b
ollama pull nomic-embed-text
ollama serve

3ï¸âƒ£ Run the app

streamlit run app.py

App will run at:
ğŸ‘‰ http://localhost:8502

ğŸ³ Run with Docker

docker build -t pdf-rag-app .
docker run -p 8501:8501 pdf-rag-app

## ğŸ› ï¸ Tech Stack

Python 3.11

LangChain

FAISS

Streamlit

Ollama + LLaMA 3

PyPDF2

![App Screenshot 1](Images/PNG1.png)
![App Screenshot 2](Images/PNG2.png)

## ğŸ‘¤ Author

Youssef Kaddam
ğŸ’¼ Data Science & AI
ğŸ”— https://github.com/YoussefKADDAM
